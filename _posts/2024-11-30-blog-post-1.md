---
title: 'Decision Tree: Building and Pruning'
date: 2024-11-30
permalink: /posts/2024/11/blog-post-1/
tags:
  - machine learning
  - tree
---

This post is a summary of how decision tree is built and pruned. The content is based on the following resources:

1. Elements of Statistical Learning, Chapter 9
2. Introduction to Statistical Learning, Chapter 8
3. 统计学习方法, Chapter 5
4. Lectures on Machine Learning (Fall 2017) by Hyeong In Choi, Seoul National University. [Lecture 9: Classification and Regression Tree (CART)](https://www.math.snu.ac.kr/~hichoi/machinelearning/lecturenotes/CART.pdf)
5. STAT 897D, Penn State University. [11.8.2 - Minimal Cost-Complexity Pruning](https://online.stat.psu.edu/stat857/node/60/)
6. STAT 451, UW Madison, [Lecture 6 Decision Trees](https://sebastianraschka.com/pdf/lecture-notes/stat451fs20/06-trees__notes.pdf)

# Building a Decision Tree

Data $$X\in\mathbb{R}^{N\times K}$$, $$Y\in\mathbb{R}^{N}$$, where$$N$$ is the number of samples and$$K$$ is the number of features. The goal is to build a decision tree that predicts$$Y$$ based on$$X$$. 

## Regression
Minimize $$\sum_{i=1}^N (y_i - \hat{y}_i)^2$$. That is, find $$ f(x) = \sum_{m=1}^M c_m I(x\in R_m)$$, where $$R_m$$ is the partitioned region, $$c_m = \text{avg}(y_i|x_i\in R_m)$$, and $$M$$ is the number of regions.

Greedy algorithm: starting with all data, consider a splitting variable$$j$$ and a splitting point$$s$$, and split the data into two regions$$R_1(j,s)$$ and$$R_2(j,s)$$. Find the best$$j$$ and$$s$$ that minimize the sum of squared errors. Repeat this process until a stopping criterion is met.

$$\min_{j, s} [\min_{c_1} \sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$$

$$= \min_{j, s} [\sum_{x_i\in R_1(j,s)}(y_i-\text{avg}(y_i|x_i\in R_1))^2 + \sum_{x_i\in R_2(j,s)}(y_i-\text{avg}(y_i|x_i\in R_2))^2]$$

This can be computed efficiently by sorting the data and calculating the cumulative sum of $$y_i$$. Prefix algorithms can be used to calculate the sum of squared errors in $$O(N)$$ time.

## Classification

At each node, calculate the impurity of the node. Common impurity measures are Gini impurity and cross-entropy. The goal is to minimize the impurity of the node. 

Why we choose Gini and cross-entropy? They are
1. Differentiable, more amenable to optimization.
2. More sensitive to node purity than misclassification error. ([6.8 Why Growing Decision Trees via Entropy or Gini Impurity
instead of Misclassification Error?
](https://sebastianraschka.com/pdf/lecture-notes/stat451fs20/06-trees__notes.pdf))

# Pruning a Decision Tree

## Cost

Let $$t$$ be a node of tree $$T$$. The cost of node $$t$$ is $$C(t)$$, and we have: $$C(t) \geq C(t_L) + C(t_R)$$.

### Regression

Regression: $$C(t) = \sum_{i}(y_i - \hat{y}_t)^2$$, where $$\hat{y}_t$$ is the prediction of node $$t$$. Then 
$$C(t) = \sum_{i}(y_i - \hat{y}_t)^2 = \sum_{i\in L}(y_i - \hat{y}_L + \hat{y}_L - \hat{y}_t)^2 + \sum_{i\in R}(y_i - \hat{y}_R+ \hat{y}_R- \hat{y}_t)^2$$
$$= \sum_{i\in L}(y_i - \hat{y}_L)^2 + \sum_{i\in R}(y_i - \hat{y}_R)^2 + \sum_{i\in L}(\hat{y}_L - \hat{y}_t)^2 + \sum_{i\in R}(\hat{y}_R - \hat{y}_t)^2$$
$$+ 2\sum_{i\in L}(y_i - \hat{y}_L)(\hat{y}_L - \hat{y}_t) + 2\sum_{i\in R}(y_i - \hat{y}_R)(\hat{y}_R - \hat{y}_t) $$
$$\geq \sum_{i\in L}(y_i - \hat{y}_L)^2 + \sum_{i\in R}(y_i - \hat{y}_R)^2 = C(t_L) + C(t_R)$$.


### Classification

$$C(t) = N_t f(p)$$, where $$p$$ is the distribution of data points in node $$t$$, $$N_t$$ is the number of data points in node $$t$$, and $$f(p)$$ is the impurity measure, such as Gini impurity, cross-entropy, or misclassification error, which should be concave. Then
$$f(p) = f(\sum p(c|i)p_i) \geq \sum p_i f(p(c|i)) = \sum p_i f(t_i)$$
$$C(t) = N_t f(p) \geq N_L f(p_L) + N_R f(p_R) = C(t_L) + C(t_R)$$.

## Cost-complexity Pruning

Let $$C_\alpha(t) = C(t)+\alpha$$, $$C\_alpha(T) = \sum_{t\in \tilde{T}} C\_alpha(t)$$, $$\tilde{T}$$ is the set of terminal nodes in tree $$T$$. The cost-complexity pruning algorithm is to find the tree $$T_\alpha$$ that minimizes $$C_\alpha(T)$$.

### Unique smallest subtree for each $$\alpha$$

For each $$\alpha$$, there is a unique smallest subtree $$T_\alpha$$ that minimizes $$C_\alpha(T)$$. Proof use the induction on the number of nodes in the tree, and can be found in [Lecture 9: Classification and Regression Tree (CART)](https://www.math.snu.ac.kr/~hichoi/machinelearning/lecturenotes/CART.pdf).

### Weakest link pruning (Breiman 1984)

For each node $$t\in T = T(\alpha_0)$$ where $$\alpha_0 = 0$$, calculate $$g(t) = \frac{C(t)-C(T_t)}{\|T_t\|-1}$$, where $$T_t$$ is the subtree rooted at node $$t$$. Find the node $$t$$ that minimizes $$g(t)$$, and prune the subtree rooted at $$t$$ to obtain pruned subtree $$T_1$$. Let $$\alpha_1=\min_{t} g(t)$$, then $$T_1=T(\alpha_1)$$ is the smallest subtree that minimizes $$C_\alpha(T)$$ for $$\alpha\in[\alpha_1, \alpha_2)$$. Repeat this process until the root node is reached. In fact, we have $$T(\alpha_j)\subset T(\alpha_i)$$ when $$\alpha_i<\alpha_j$$. Proof can be found in [Lecture 9: Classification and Regression Tree (CART)](https://www.math.snu.ac.kr/~hichoi/machinelearning/lecturenotes/CART.pdf).

## Cross-validation

After obtained the sequence of subtrees $$T(\alpha)$$, we can use cross-validation to select the best $$\alpha$$. 

## Complexity

$$C(t)$$: The cost of node $$t$$. This only needs to be computed once for each node. $$O(N)$$.
$$C(T_t)$$: The cost of the subtree rooted at node $$t$$. This needs to be updated when pruning the tree.
$$T_t$$: The subtree rooted at node $$t$$. This needs to be updated when pruning the tree.

Since $$C(T_t)$$ and $$T_t$$ will only be updated when pruning the tree, the complexity of pruning is $$O(N)$$.