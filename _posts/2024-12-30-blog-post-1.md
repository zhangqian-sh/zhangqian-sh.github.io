---
title: 'Spectral Theorem and Singular Value Decomposition'
date: 2024-12-30
permalink: /posts/2024/12/blog-post-1/
tags:
  - machine learning
  - linear algebra
---

This post is a summary of the proof of the spectral theorem and singular value decomposition. The content is based on the following resources:

1. [Lecture 2m](https://www.math.uwaterloo.ca/~jmckinno/Math225/Week7/Lecture2m.pdf)
2. [Note 14: Symmetric Matrices and Spectral Theorem](https://eecs16b.org/notes/sp24/note14.pdf)
3. [Spectral Theorems for Hermitian and unitary matrices](https://www.math.purdue.edu/~eremenko/dvi/spectral.pdf)
4. [Harvard MATH22B 2019 Lectrue 16](https://people.math.harvard.edu/~knill/teaching/math22b2019/handouts/lecture16.pdf)
5. [Harvard MATH22B 2019 Lectrue 17](https://people.math.harvard.edu/~knill/teaching/math22b2019/handouts/lecture17.pdf)
6. [Proof of the Singular Value Decomposition](https://gregorygundersen.com/blog/2018/12/20/svd-proof/)
7. 统计学习方法第2版, Chapter 15


# Spectral Theorem

The statement of the spectral theorem is as follows:

Suppose $$A$$ is an $$n \times n$$ symmetric matrix. Then there exists an orthogonal matrix $$P$$ and a diagonal matrix $$D$$ such that $$P^T AP = D$$. That is, every symmetric matrix is orthogonally diagonalizable.

There are three steps to prove the spectral theorem:

1. Prove that the eigenvalues of a symmetric matrix are real.
2. Eigenvectors corresponding to distinct eigenvalues are orthogonal.
3. The eigenvectors of a symmetric matrix form an orthogonal basis (Diagnolization).

The proof of [Lecture 2m](https://www.math.uwaterloo.ca/~jmckinno/Math225/Week7/Lecture2m.pdf) uses induction and is elementary. [Note 14: Symmetric Matrices and Spectral Theorem](https://eecs16b.org/notes/sp24/note14.pdf) gives some intuition, but there is a gap because it does not discuss the existence of the orthogonal basis. [Spectral Theorems for Hermitian and unitary matrices](https://www.math.purdue.edu/~eremenko/dvi/spectral.pdf) uses the restriction of linear operator. It is rigorous but abstract.

This theorem is a special case of diagonalization. [Harvard MATH22B 2019 Lectrue 16](https://people.math.harvard.edu/~knill/teaching/math22b2019/handouts/lecture16.pdf) gives the sufficient and necessary conditions for diagnolization of general matrices, and [Harvard MATH22B 2019 Lectrue 17](https://people.math.harvard.edu/~knill/teaching/math22b2019/handouts/lecture17.pdf) gives the proof of the spectral theorem using some continuity arguments (and induction, like in [Lecture 2m](https://www.math.uwaterloo.ca/~jmckinno/Math225/Week7/Lecture2m.pdf)).

# Singular Value Decomposition
Singular Value Decomposition (SVD) is a generalization of the spectral theorem. The statement of SVD is as follows:

Suppose $$A$$ is an $$m \times n$$ matrix. Then there exists an orthogonal matrix $$U$$, a diagonal matrix $$\Sigma$$, and an orthogonal matrix $$V$$ such that $$A = U \Sigma V^T$$. That is, every matrix can be decomposed into three matrices.

This is a very strong statement (every matrix), but its constructive proof is quite clear. There are some important linear algebra facts to keep in mind when proving SVD:

1. Spectral theorem: Real symmetric matrices are orthogonally diagonalizable.
2. Semi-positive definite matrices have non-negative eigenvalues.
3. $$rank(A) = rank(A^T) = rank(AA^T) = rank(A^TA)$$.

## Proof
Here is the proof of SVD:

### Construct $$V$$ and $$\Sigma$$
Consider $$m\times n$$ matrix $$A$$ with rank $$r$$. WLOG, assume $$r\leq n$$. $$A^T A$$ is SPD and thus diagnolizable with nonnegative eigenvectors. Let $$v_1, v_2, \ldots, v_n$$ be the eigenvectors of $$A^T A$$ corresponding eigenvalues $$\sigma_1^2, \sigma_2^2, \ldots, \sigma_n^2$$. Then $$v_1, v_2, \ldots, v_n$$ form an orthogonal basis of $$\mathbb{R}^n$$. Due to fact 3, there are $$r$$ positive eigenvalues $$\sigma_1^2, \sigma_2^2, \ldots, \sigma_r^2$$ and $$n-r$$ zero eigenvalues. Let $$v_1, v_2, \ldots, v_r$$ be the eigenvectors corresponding to the positive eigenvalues. Let $$V=[V_1, V_2]$$, where $$V_1 = [v_1, v_2, \ldots, v_r]$$ and $$V_2 = [v_{r+1}, v_{r+2}, \ldots, v_n]$$.

Let
$$\Sigma_1 = \begin{bmatrix}
\sigma_1 & 0 & \ldots & 0 \\
0 & \sigma_2 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \sigma_r \\
\end{bmatrix}$$ and $$\Sigma=\begin{bmatrix}
\Sigma_1 & 0 \\
0 & 0
\end{bmatrix}$$.

### Filter $$A$$ with $$V_1$$
$$A^T Av_j=0$$ for $$j=r+1, \ldots, n$$. Due to fact 3 (see the details), $$N(A^T A)= N(A)$$, and $$Av_j=0$$ for $$j=r+1, \ldots, n$$. Since $$V$$ is orthonormal, $$I = VV^T = V_1V_1^T + V_2V_2^T$$, and thus $$A = AVV^T = AV_1V_1^T + AV_2V_2^T = AV_1V_1^T$$.

### Construct $$U$$
Let $$u_j = \frac{Av_j}{\sigma_j}$$, $$j=1,2,\ldots,r$$, $$U = [u_1, \ldots, u_r]$$. Then $$AV_1=U\Sigma_1$$, $$u_j$$ is a unit eigenvalue of $$AA^T$$, because
$$AA^Tu_j = A (A^T Av_j) / \sigma_j = \sigma^2 u_j$$, and $$u_j^T u_j = \frac{v_j^TA^TAv_j}{\sigma_j^2}=1$$.

For $$r<n$$, we just need to complete $$U$$ by additional orthonormal vectors to span $$\mathbb{R}^m$$ and thus $$AV=U\Sigma$$, which leads to the SVD of $$A$$.

## Details
1. Discussed above.
2. Let $$A$$ be SPD matrix. Then for its eigenvalue $$\lambda$$ and eigenvector $$x$$, $$Ax=\lambda x$$, and $$x^TAx=\lambda x^Tx=\lambda \|x\|^2\geq 0$$. Thus $$\lambda\geq 0$$.
3. $$N(A)\subset N(A^T A)$$ is obvious. For the reverse, consider $$A^TAx=0$$, then $$x^TA^Ax=0$$, hence $$\|Ax\|^2=0$$, and $$Ax=0$$. Thus $$N(A^TA)=N(A)$$. So $$rank(A)=rank(A^TA)$$.