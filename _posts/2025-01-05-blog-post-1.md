---
title: 'Principal Component Analysis from Principle'
date: 2025-01-05
permalink: /posts/2025/01/blog-post-1/
tags:
  - machine learning
  - linear algebra
---

This post is a summary of the deduction of the principal component analysis (PCA) from two principles:
1. Maximize variance
2. Minimize reconstruction error

The content is based on the following resources:
1. [An Introduction to Statistical Learning (ISL)](https://www.statlearning.com/)
2. [CMU Undergraduate Advanced Data Analysis (36-402)](https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf)
3. [Slides on Principal Component Analysis, Frank Wood](http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/pca.pdf)
4. 统计学习方法第2版, Chapter 16

# Maximize Variance

## First Principal Component
The first principal component of a dataset $$(X_1, X_2, ,\ldots, X_p)$$ is a linear combination of the features $$Z_1 = \sum_{j=1}^p \alpha_{j1}X_j=X\alpha_1$$ that has the largest variance subject to the constraint that $$\sum_{j=1}^p \alpha_{j1}^2 = 1$$.

To solve this constrained optimization problem, we use the method of Lagrange multipliers. The Lagrangian is

$$\mathcal{L(\alpha_1, \lambda)} = \text{Var}(X\alpha_1) - \lambda(\alpha_1^T\alpha_1 - 1) = \alpha_1^TX^TX\alpha_1 - \lambda(\alpha_1^T\alpha_1 - 1)$$

Differentiating with respect to $$\alpha_1$$ and setting the result to zero, we get

$$\frac{\partial \mathcal{L}}{\partial \alpha_1} = 2X^TX\alpha_1 - 2\lambda\alpha_1 = 0$$

Thus $$\alpha_1$$ is a unit eigenvector of $$X^TX$$ and $$\lambda$$ is the corresponding eigenvalue. To maximize the variance, we choose $$\alpha_1$$ to be the eigenvector corresponding to the largest eigenvalue $$\lambda_1$$.

## $$m$$-th Principal Component

The $$m$$-th principal component is a linear combination of the features $$Z_m = \sum_{j=1}^p \alpha_{jm}X_j = X\alpha_m$$ that has the largest variance subject to the constraint that $$\sum_{j=1}^p \alpha_{jm}^2 = 1$$ and $$\text{Cov}(Z_i, Z_m) = 0$$ for $$i < m$$. This adds more constraints to the optimization problem. For simplicity, we discuss the case where $$m=2$$, and the general case can be computed similarly.

$$\text{Cov}(Z_1, Z_2) = \text{Cov}(X\alpha_1, X\alpha_2) = \alpha_1^TX^TX\alpha_2 = \lambda_1\alpha_1^T\alpha_2 = 0$$ because $$\alpha_1$$ is the eigenvector of $$X^TX$$.

The Lagrangian is

$$\mathcal{L(\alpha_2, \lambda, \gamma)} = \text{Var}(X\alpha_2) - \lambda(\alpha_2^T\alpha_2 - 1) - \gamma(\alpha_1^TX^TX\alpha_2) = \alpha_2^TX^TX\alpha_2 - \lambda(\alpha_2^T\alpha_2 - 1) - \gamma(\alpha_1^T\alpha_2)$$

Differentiating with respect to $$\alpha_2$$ and setting the result to zero, we get

$$\frac{\partial \mathcal{L}}{\partial \alpha_2} = 2X^TX\alpha_2 - 2\lambda\alpha_2 - \gamma\alpha_1 = 0$$

Left-multiplying by $$\alpha_1^T$$ and using $$\alpha_1^T\alpha_2 = 0$$, we get $$\gamma = 0$$. Thus $$\alpha_2$$ is a unit eigenvector of $$X^TX$$ and $$\lambda_2$$ is the corresponding eigenvalue. To maximize the variance, we choose $$\alpha_2$$ to be the eigenvector corresponding to the second largest eigenvalue $$\lambda_2$$ ($$\lambda_2$$ may equal to $$\lambda_1$$, but this does not matter, we just need $$\alpha_2$$ and $$\alpha_1$$ to be orthogonal).

# Minimize Reconstruction Error
There are two ways to interpret PCA as minimizing reconstruction error:
1. Matrix completion: For a $$n\times p$$ matrix, find $$n\times M$$ matrix and $$M\times p$$ matrix that minimizes the Frobenius norm of the difference between the original matrix and the product of the two matrices. That is, $$\min_{Z\in\mathbb{R}^{n\times M}, A\in\mathbb{R}^{p\times M}} \|X - ZA^T\|_F^2$$ s.t. $$A^TA = I$$.
2. Closest subspace: For $$p$$-dimensional dataset, find $$M$$-dimensional subspace ($$M\leq p$$) that minimizes the sum of squared perpendicular distances from the data points to the subspace. Then $$A$$ is the orthogonal basis of the subspace.

Now we solve the optimization problem to prove it is equivalent to the PCA problem.

## Proof

### Find $$Z$$ given $$A$$
Given $$A$$, we want to find $$Z$$ that minimizes $$\|X - ZA^T\|_F^2$$.

$$\|X - ZA^T\|_F^2 = \sum_{i,j} (X_{ij} - \sum_{m=1}^M Z_{im}A_{jm})^2$$. Differentiating with respect to $$Z_{kl}$$ and setting the result to zero, we get

$$\frac{\partial \|X - ZA^T\|_F^2}{\partial Z_{kl}} = -2\sum_{j=1}^p (X_{kj} - \sum_{m=1}^M Z_{km}A_{jm})A_{jl} = 0$$

$$ZA^T A = XA$$, hence $$Z = XA$$.

### Solve the Simplified Problem
Now the problem is reduced to $$\min_{A\in\mathbb{R}^{p\times M}} \|X - XAA^T\|_F^2$$ s.t. $$A^TA = I$$.

$$
\begin{aligned} 
\|X - XAA^T\|_F^2 &= \sum_i \|X^T_i-X^T_i AA^T\|_2^2 \\
&= \sum_i (X^T_i - X^T_i AA^T)(X^T_i - X^T_i AA^T)^T \\
&= \sum_i X^T_i(X^T_i)^T - X^T_iAA^T(X^T_i)^T - X^T_iAA^T(X^T_i)^T + X^T_iAA^TAA^T(X^T_i)^T \\
& = \sum_i X^T_i(X^T_i)^T - X^T_iAA^T(X^T_i)^T \\
\end{aligned}
$$

where $$X^T_i$$ is the $$i$$-th column of $$X^T$$ ($$i$$-th row of $$X$$). Then the minimization problem becomes the maximization problem

$$\max_{A\in\mathbb{R}^{p\times M}}\sum_i X^T_iAA^T(X^T_i)^T = \max_{A\in\mathbb{R}^{p\times M}}\|XA\|_F^2$$.

### Equivalence to the maximum variance
The problem is equivalent to the maximum variance problem because $$\|XA\|_F^2  = \sum_{m=1}^M \|X\alpha_m\|_2^2$$ where $$\alpha_m$$ is the $$m$$-th column of $$A$$. Then we can maximize in a greedy way with the orthonormal constraint.

# Computation
The computation of PCA can be done by the following steps:
1. Standardize the data.
2. Compute the eigenvectors and eigenvalues of the covariance matrix $$X^T X$$. Sort the eigenvalues in decreasing order.
3. Choose the $$M$$ eigenvectors corresponding to the $$M$$ largest eigenvalues to be the columns of $$A$$.
4. Transform the data into the new subspace by $$Z = XA$$.

However, computing the eigenvalues of $$X^T X$$ can be unstable. An example is the Läuchli matrix,

$$X = \begin{bmatrix}
1 & 1 \\
\epsilon & 0 \\
0 & \epsilon
\end{bmatrix}$$

where $$\epsilon$$ is a small number. Then 

$$X^T X = \begin{bmatrix}
1+\epsilon^2 & 1 \\
1 & 1+\epsilon^2
\end{bmatrix}$$

and the eigenvalues are $$2+\epsilon^2$$ and $$\epsilon^2$$. But when $$\epsilon$$ is small, computer may give $$2$$ and $$0$$ as the eigenvalues. Since $$X^T X$$ ill-conditioned, computation can be unstable. A better way is to use the singular value decomposition (SVD) of $$X = UDV^T$$ where $$U$$ and $$V$$ are orthogonal matrices and $$D$$ is a diagonal matrix. Then $$X^T X = VD^2V^T$$ and the eigenvectors of $$X^T X$$ are the columns of $$V$$.